{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training. Data has alreday been setup - test_sentiment140.csv_noquotes.csv & training_sentiment140_noquotes.csv\n",
    "\n",
    "Exmplanation of Hyperparameters here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::459367279383:role/service-role/AmazonSageMaker-ExecutionRole-20190801T095874\n",
      "venkat1-ml-sagemaker\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = 'BUCKET_NAME' #sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'Sentiment140/supervised' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 403 ms, total: 1.63 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='training_sentiment140_noquotes.csv', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='test_sentiment140.csv_noquotes.csv', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is where we Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=45,   #tried 10 and 20\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.001, #tried 0.05 and 0.02\n",
    "                            vector_dim=100,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-16 14:49:53 Starting - Starting the training job...\n",
      "2019-12-16 14:49:54 Starting - Launching requested ML instances......\n",
      "2019-12-16 14:50:58 Starting - Preparing the instances for training...\n",
      "2019-12-16 14:51:49 Downloading - Downloading input data...\n",
      "2019-12-16 14:52:13 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 WARNING 139714800158528] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 WARNING 139714800158528] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 INFO 139714800158528] nvidia-smi took: 0.0251891613007 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 INFO 139714800158528] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 INFO 139714800158528] Processing /opt/ml/input/data/train/training_sentiment140_noquotes.csv . File size: 93 MB\u001b[0m\n",
      "\u001b[34m[12/16/2019 14:52:26 INFO 139714800158528] Processing /opt/ml/input/data/validation/test_sentiment140.csv_noquotes.csv . File size: 40 MB\u001b[0m\n",
      "\u001b[34mRead 10M words\u001b[0m\n",
      "\u001b[34mRead 16M words\u001b[0m\n",
      "\u001b[34mNumber of words:  178971\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/test_sentiment140.csv_noquotes.csv\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0009  Progress: 7.27%  Million Words/sec: 15.47 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.780233\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0009  Progress: 12.27%  Million Words/sec: 13.83 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.787494\u001b[0m\n",
      "\n",
      "2019-12-16 14:52:25 Training - Training image download completed. Training in progress.\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.792406\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0008  Progress: 17.40%  Million Words/sec: 12.03 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.795585\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.799288\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.802013\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0008  Progress: 22.59%  Million Words/sec: 10.60 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.803898\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.805696\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0007  Progress: 27.73%  Million Words/sec: 10.30 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 13\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.808163\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 14\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.809477\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0007  Progress: 32.84%  Million Words/sec: 10.10 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 15\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.810742\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.811054\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0006  Progress: 37.94%  Million Words/sec: 9.96 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 17\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.812558\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.81285\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 19\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.813827\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0006  Progress: 43.14%  Million Words/sec: 9.60 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 20\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.814656\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 21\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.815254\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0005  Progress: 48.24%  Million Words/sec: 9.55 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 22\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.815265\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 23\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.815723\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0005  Progress: 53.37%  Million Words/sec: 9.51 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 24\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.815792\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 25\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816435\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 26\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816777\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0004  Progress: 58.50%  Million Words/sec: 9.32 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 27\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816771\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 28\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816983\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0004  Progress: 63.68%  Million Words/sec: 9.41 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 29\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817152\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 30\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816975\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0003  Progress: 68.77%  Million Words/sec: 9.49 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 31\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.816869\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 32\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817858\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 33\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817648\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0003  Progress: 73.97%  Million Words/sec: 9.50 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 34\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817423\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 35\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817585\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0002  Progress: 79.03%  Million Words/sec: 9.65 #####\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m-------------- End of epoch: 36\u001b[0m\n",
      "\u001b[34mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.817496\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[34mReached patience. Terminating training.\u001b[0m\n",
      "\u001b[34mBest epoch: 32\u001b[0m\n",
      "\u001b[34mBest validation accuracy: 0.817858\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 12.04 #####\u001b[0m\n",
      "\n",
      "2019-12-16 14:53:48 Uploading - Uploading generated training model\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 12.04\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 61.47\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.8819\u001b[0m\n",
      "\u001b[34mNumber of train examples: 1120000\n",
      "\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.8179\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 480000\u001b[0m\n",
      "\n",
      "2019-12-16 14:55:30 Completed - Training job completed\n",
      "Training seconds: 221\n",
      "Billable seconds: 221\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.7255306243896484,\n",
      "      0.2744893729686737\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.717772364616394,\n",
      "      0.2822476327419281\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.7200742959976196,\n",
      "      0.2799457311630249\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.7346668243408203,\n",
      "      0.26535317301750183\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8802874088287354,\n",
      "      0.11973258852958679\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8215214610099792,\n",
      "      0.1784985214471817\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.6033281087875366,\n",
      "      0.3966919183731079\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9527021646499634,\n",
      "      0.047317855060100555\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8121321201324463,\n",
      "      0.18788792192935944\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8851485252380371,\n",
      "      0.11487144976854324\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8036215305328369,\n",
      "      0.19639843702316284\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.8969753980636597,\n",
      "      0.10304461419582367\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.7934820055961609,\n",
      "      0.20653806626796722\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Positive\",\n",
      "      \"__label__Negative\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.6296713948249817,\n",
      "      0.3703486919403076\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__Negative\",\n",
      "      \"__label__Positive\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = [\"One does not simply walk into Mordor\",\n",
    "             \"One does not simply walk into Chicago\",\n",
    "             \"Hey, Connor. Money in Hand is good, money in head is bad\",\n",
    "            \"what was he thinking?\", \n",
    "             \"morning all... looks like its gonna be a rainy day \",\n",
    "            \"hard to be witty when one is at their wits end\",\n",
    "            \"Crazy day tomorow. Meetings all day\",\n",
    "            \"looking forward to the weekend\",\n",
    "            \"A person called asking for Information\",\n",
    "            \"A HCP called asking for Information\",\n",
    "            \"A HCP called asking for Information on ProductX\",\n",
    "            \"A HCP called asking for Information on QETUOD\",\n",
    "            \"A HCP called and told us to go fly a kite\",\n",
    "            \"A HCP called and told us to go to hell\"]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "#tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "# NOTE: Earlier version of code had tokenizer. Bad idea for sentiment analysis since the placement of words matters. \n",
    "# keeping commented code here as a reminder\n",
    "\n",
    "payload = {\"instances\" : sentences,\n",
    "          \"configuration\": {\"k\": 2}}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(text_classifier.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
